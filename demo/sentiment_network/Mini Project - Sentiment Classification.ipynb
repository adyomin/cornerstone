{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "Material by Andrew Trask\n",
    "\n",
    "- **Twitter**: @iamtrask\n",
    "- **Blog**: http://iamtrask.github.io\n",
    "\n",
    "Edits & solution by Andrei Dyomin\n",
    "\n",
    " - **GitHub**: https://github.com/adyomin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Start with your neural network from the last chapter\n",
    "- 3 layer neural network\n",
    "- no non-linearity in hidden layer\n",
    "- use our functions to create the training data\n",
    "- create a \"pre_process_data\" function to create vocabulary for our training data generating functions\n",
    "- modify \"train\" to train over the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "nn_path = '/Users/adyomin/Yandex.Disk.localized/Projects/Cornerstone'\n",
    "sys.path.append(nn_path)\n",
    "import network as nn\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "f = open('./data/reviews.txt','r')\n",
    "reviews = [line.strip('\\n') for line in f]\n",
    "f.close()\n",
    "\n",
    "f = open('./data/labels.txt','r')\n",
    "text_labels = [line.strip('\\n').upper() for line in f]\n",
    "f.close()\n",
    "\n",
    "f = open('./data/full_stop_list.txt','r')\n",
    "full_stop_list = [line.strip('\\n') for line in f]\n",
    "f.close()\n",
    "\n",
    "total_counts = Counter()\n",
    "for review in reviews:\n",
    "        total_counts.update(re.findall('\\w{3,}', review.lower()))\n",
    "for word in full_stop_list:\n",
    "    del total_counts[word]\n",
    "\n",
    "vocab = set(total_counts.keys())\n",
    "word2index = Counter()\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "batch_size = 512\n",
    "eta = 0.01\n",
    "n_records = len(text_labels)\n",
    "n_features = len(vocab)\n",
    "features = np.zeros(shape=(batch_size, n_features))\n",
    "labels = np.zeros(shape=(batch_size, 1))\n",
    "nn_model = nn.Network((73297, 1024, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    for i in range(0, n_records, batch_size):\n",
    "        features *= 0\n",
    "        labels *= 0\n",
    "        for j, review in enumerate(reviews[i:i + batch_size]):\n",
    "            for word in review.split():\n",
    "                if word in vocab:\n",
    "                    features[j, word2index[word]] += 1\n",
    "        for k, label in enumerate(text_labels[i:i + batch_size]):\n",
    "            if label == 'POSITIVE':\n",
    "                labels[k, 0] = 1\n",
    "            else:\n",
    "                labels[k, 0] = 0\n",
    "        nn_model.train_single_loop(features, labels, batch_size, eta)\n",
    "        percentage = epoch/10*100\n",
    "        t_loss = nn_model.evaluate(x=features, y=labels)\n",
    "        print('Progress: {0:.2f}%, Training loss: {1:.4f}'.format(percentage,\n",
    "                                                              t_loss))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
