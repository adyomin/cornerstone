{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "Material by Andrew Trask\n",
    "\n",
    "- **Twitter**: @iamtrask\n",
    "- **Blog**: http://iamtrask.github.io\n",
    "\n",
    "Edits & solution by Andrei Dyomin\n",
    "\n",
    " - **GitHub**: https://github.com/adyomin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Should Already Know\n",
    "\n",
    "- neural networks, forward and back-propagation\n",
    "- stochastic gradient descent\n",
    "- mean squared error\n",
    "- and train/test splits\n",
    "\n",
    "### Tutorial Outline:\n",
    "\n",
    "- Intro: The Importance of \"Framing a Problem\"\n",
    "\n",
    "\n",
    "- Curate a Dataset\n",
    "- Developing a \"Predictive Theory\"\n",
    "- **PROJECT 1**: Quick Theory Validation\n",
    "\n",
    "\n",
    "- Transforming Text to Numbers\n",
    "- **PROJECT 2**: Creating the Input/Output Data\n",
    "\n",
    "\n",
    "- Putting it all together in a Neural Network\n",
    "- **PROJECT 3**: Building our Neural Network\n",
    "\n",
    "\n",
    "- Understanding Neural Noise\n",
    "- **PROJECT 4**: Making Learning Faster by Reducing Noise\n",
    "\n",
    "\n",
    "- Analyzing Inefficiencies in our Network\n",
    "- **PROJECT 5**: Making our Network Train and Run Faster\n",
    "\n",
    "\n",
    "- Further Noise Reduction\n",
    "- **PROJECT 6**: Reducing Noise by Strategically Reducing the Vocabulary\n",
    "\n",
    "\n",
    "- Analysis: What's going on in the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "56bb3cba-260c-4ebe-9ed6-b995b4c72aa3"
    }
   },
   "source": [
    "# Lesson: Curate a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eba2b193-0419-431e-8db9-60f34dd3fe83"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "f = open('./data/reviews.txt','r') # What we know!\n",
    "reviews = [line.strip('\\n') for line in f]\n",
    "f.close()\n",
    "\n",
    "f = open('./data/labels.txt','r') # What we WANT to know!\n",
    "labels = [line.strip('\\n').upper() for line in f]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "bb95574b-21a0-4213-ae50-34363cf4f87f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e0408810-c424-4ed4-afb9-1735e9ddbd0a"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Develop a Predictive Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e67a709f-234f-4493-bae6-4fb192141ee0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.txt \t : \t reviews.txt\n",
      "\n",
      "NEGATIVE\t:\tthis movie is terrible but it has some good effects .  ...\n",
      "POSITIVE\t:\tadrian pasdar is excellent is this film . he makes a fascinating woman .  ...\n",
      "NEGATIVE\t:\tcomment this movie is impossible . is terrible  very improbable  bad interpretat...\n",
      "POSITIVE\t:\texcellent episode movie ala pulp fiction .  days   suicides . it doesnt get more...\n",
      "NEGATIVE\t:\tif you haven  t seen this  it  s terrible . it is pure trash . i saw this about ...\n",
      "POSITIVE\t:\tthis schiffer guy is a real genius  the movie is of excellent quality and both e...\n"
     ]
    }
   ],
   "source": [
    "print(\"labels.txt \\t : \\t reviews.txt\\n\")\n",
    "pretty_print_review_and_label(2137)\n",
    "pretty_print_review_and_label(12816)\n",
    "pretty_print_review_and_label(6267)\n",
    "pretty_print_review_and_label(21934)\n",
    "pretty_print_review_and_label(5297)\n",
    "pretty_print_review_and_label(4998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Quick Theory Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using Python's Regular Expressions library to filter out all 'small' words\n",
    "\n",
    "for review, label in zip(reviews, labels):\n",
    "    if label == 'POSITIVE':\n",
    "            positive_counts.update(re.findall('\\w{3,}', review.lower()))\n",
    "    else:\n",
    "            negative_counts.update(re.findall('\\w{3,}', review.lower()))\n",
    "\n",
    "total_counts.update(positive_counts)\n",
    "total_counts.update(negative_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a stop list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out https://en.wikipedia.org/wiki/Stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_list_general - 119 words\n",
      "stop_list_reviews - 291 words\n"
     ]
    }
   ],
   "source": [
    "f = open('./data/stop_list.csv','r')\n",
    "stop_list_general = set(f.read().split(', '))\n",
    "f.close()\n",
    "print('stop_list_general - {0} words'.format(len(stop_list_general)))\n",
    "\n",
    "stop_list_reviews = set()\n",
    "for word, count in total_counts.most_common():\n",
    "    if count >= 2000:\n",
    "        stop_list_reviews.add(word)\n",
    "len(stop_list_reviews)\n",
    "\n",
    "print('stop_list_reviews - {0} words'.format(len(stop_list_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# super subjective list\n",
    "keep_words = set('bad, dvd, nothing, old, pretty, again, beautiful, best, big, excellent, first, fun, funny, good, great, horror, interesting, little, like, love, many, money, most, never, must, nice, off, original, point, special, star, true, very, well, worst'.split(', '))\n",
    "len(keep_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_stop_list = stop_list_general | stop_list_reviews - keep_words\n",
    "len(full_stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f = open('./data/full_stop_list.txt', 'r+')\n",
    "# for word in full_stop_list:\n",
    "#     f.writelines('{0}'.format(word))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in full_stop_list:\n",
    "    del positive_counts[word]\n",
    "    del negative_counts[word]\n",
    "    del total_counts[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_neg_ratios = Counter()\n",
    "threshold = 256\n",
    "max_ratio = 3\n",
    "max_ratio_raw = np.exp(max_ratio)\n",
    "\n",
    "for word, count in total_counts.most_common():\n",
    "    if (count > threshold) & (negative_counts[word] > 0) :\n",
    "        pos_neg_ratio = positive_counts[word]/negative_counts[word]\n",
    "    elif (count > threshold) & (negative_counts[word] == 0) :\n",
    "        pos_neg_ratio = max_ratio_raw\n",
    "    pos_neg_ratios[word] = pos_neg_ratio\n",
    "    \n",
    "for word, ratio in pos_neg_ratios.most_common():\n",
    "    if ratio > 0:\n",
    "        pos_neg_ratios[word] = np.log(ratio)\n",
    "    elif ratio == 0:\n",
    "        print (word)\n",
    "        pos_neg_ratios[word] = -max_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wonderfully', 2.0485643031153966),\n",
       " ('delightful', 1.8262456452992242),\n",
       " ('beautifully', 1.7784436932522829),\n",
       " ('superb', 1.7189076208420597),\n",
       " ('touching', 1.6514021115331325),\n",
       " ('stewart', 1.6249021381316819),\n",
       " ('friendship', 1.588384503236268),\n",
       " ('magnificent', 1.5686159179138452),\n",
       " ('wonderful', 1.5680329974659779),\n",
       " ('finest', 1.5668782980153044)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most frequently seen in a review with a \"POSITIVE\" label\n",
    "pos_neg_ratios.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unfunny', -2.6882475738060303),\n",
       " ('waste', -2.6186484579840514),\n",
       " ('pointless', -2.4531579514734201),\n",
       " ('redeeming', -2.3648889763302003),\n",
       " ('worst', -2.2865847516476046),\n",
       " ('laughable', -2.2617630984737906),\n",
       " ('awful', -2.2265521924307397),\n",
       " ('poorly', -2.2192034840549946),\n",
       " ('sucks', -1.9830278120118159),\n",
       " ('lame', -1.9802348915963879)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most frequently seen in a review with a \"NEGATIVE\" label\n",
    "list(reversed(pos_neg_ratios.most_common()))[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Text into Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**review** = \"This was a horrible, terrible movie.\"\n",
    "\n",
    "<img src = './data/sentiment_network.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**review** = \"The movie was excellent\"\n",
    "\n",
    "<img src = './data/sentiment_network_pos.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 2: Creating the Input/Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73297\n"
     ]
    }
   ],
   "source": [
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_0 = np.zeros((1,vocab_size))\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src = './data/sentiment_network.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.2 ms, sys: 14.5 ms, total: 38.6 ms\n",
      "Wall time: 38.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "limit = 127\n",
    "\n",
    "features = np.zeros((limit, vocab_size))\n",
    "for i, review in enumerate(reviews[:limit]):\n",
    "    for word in review.split():\n",
    "        if word in vocab:\n",
    "            features[i, word2index[word]] += 1\n",
    "            \n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 73297)\n",
      "(127, 1)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "labels = np.array(labels[:limit], ndmin=2).T\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Start with your neural network from the last chapter\n",
    "- 3 layer neural network\n",
    "- no non-linearity in hidden layer\n",
    "- use our functions to create the training data\n",
    "- create a \"pre_process_data\" function to create vocabulary for our training data generating functions\n",
    "- modify \"train\" to train over the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "nn_path = '/Users/adyomin/Yandex.Disk.localized/Projects/Cornerstone'\n",
    "sys.path.append(nn_path)\n",
    "import network as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module network:\n",
      "\n",
      "__init__(self, size, h_activation='sigmoid', o_activation='pass_input', c_function='quadratic', weights=None)\n",
      "    Network class constructor method.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    size : tuple\n",
      "        size[0] - n_features, input layers width.  size[-1] - n_targets,\n",
      "        width of the output layer.\n",
      "    \n",
      "    h_activation : string\n",
      "        Choice of activation function for all hidden layers.  Current\n",
      "        options include:\n",
      "         - 'sigmoid' - returns 1/(1 + numpy.exp(-x))\n",
      "         - 'pass_input' - returns x\n",
      "    \n",
      "    o_activation : string\n",
      "        Choice of output layer activation function.  Current options\n",
      "        include:\n",
      "         - 'sigmoid' - returns 1/(1 + numpy.exp(-x))\n",
      "         - 'pass_input' - returns x\n",
      "    \n",
      "    c_function : string\n",
      "        Choice of cost/loss/objective function prime for the network.\n",
      "        Current options include:\n",
      "         - 'quadratic' - returns error, (0.5*(error**2))' = error\n",
      "    \n",
      "    weights : numpy.array - optional\n",
      "        Create a network instance using previously saved weights.\n",
      "        Dimensions control is on the user atm.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Network.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_model = nn.Network((73297, 256, 64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module network:\n",
      "\n",
      "train(self, x_train, y_train, batch_size, eta, n_epochs, shuffle=True)\n",
      "    Trains the instance with its current weights to predict y from x.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x_train : numpy.array\n",
      "        Train examples matrix of size (n_records, n_features).  Matrix\n",
      "        elements are expected to be real numbers with mean = 0.0, scaled\n",
      "        to 1.0 standard deviation.\n",
      "    \n",
      "    y_train : numpy.array\n",
      "        Train labels matrix of size (n_records, n_targets).\n",
      "    \n",
      "    batch_size : int\n",
      "        Defines how many train examples will be taken for the next step of\n",
      "        the weights update.  Would be great to add some intuition how to\n",
      "        choose a batch size.  TBD I guess.\n",
      "    \n",
      "    n_epochs : int\n",
      "        Number of epochs to train over the whole x.\n",
      "    \n",
      "    eta : float\n",
      "        Weight update step multiple.  Constant only ATM.\n",
      "    \n",
      "    shuffle: bool\n",
      "        Determines whether or not data is being shuffled each epoch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Network.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nn_model.train(features[:128], labels[:128], batch_size=16, eta=0.01, n_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
